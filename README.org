* run
** Start spider
$ scrapy crawl books -o books.csv
* concept
如果 把 框架 中 的 组件 比作 人体 的 各个 器官,
Request 和 Response 对象 是 血液, Item 则 是 代谢产物
* API
#+BEGIN_SRC python
req = Request(url[,
              callback,
              method='GET',
              headers,
              body,
              cookies,
              meta,
              encoding='utf-8',
              priority=0,
              dont_filter=False,
              errback])

import scrapy
request = scrapy.Request('http://books.toscrape.com/')
request2 = scrapy.Request('http://quotes.toscrape.com/', callback=self.parseItem)

在实际应用中, 我们几乎只调用Request的构造器创建对象,
但也可以根据需求访问Request对象的属性, 常用的有以下几个:

- url
- method
- headers
- body
- meta

这些属性和构造器参数相对应
#+END_SRC

#+BEGIN_SRC python
# response has 3 types: TextResponse, HtmlResponse, XmlResponse
#
# HtmlResponse
#
# property:
# - url, status, headers, body, text, encoding, request, meta, selector, xpath, css, urljoin

response.headers.get('Content-Type')
response.headers.getlist('Set-Cookie')

reponse.text = response.body.decode(response.encoding)

# resonse.meta 即response.request.meta
# 在构造Request对象时, 可将要传递给parse函数 的 信息 通过meta参数 传入;
# parse 函数处理响应时, 通过response.meta将信息取出
#+END_SRC
* workflow
** narrate
实现 一个 Spider 子类 的 过程, 很像 是 完成 一系列 填空题:

Scrapy 框架 提出 以下 问题, 让 用户 在 Spider 子类 中 作答:
1. 爬虫 从 哪个 或 哪些 页面 开始 爬取?
2. 对于 一个 已下载 的 页面, 提取 其中 的 哪些 数据?
3. 爬取 完 当前 页面 后, 接下来 爬取 哪个 或 哪些 页面?

#+BEGIN_SRC python
class BooksSpider(scrapy.Spider):
    start_urls = ['http://books.toscrape.com/']

    def start_requests(self):
        for url in start_urls:
            yield scrapy.Request(url,
                                 callback=self.parse_book,
                                 headers={'User-Agent': 'Mozilla/5.0'},
                                 dont_filter=True)

    def parse_book(response):
        yield {}
        yield scrapy.Request(next_url, callback=self.parse_book)
#+END_SRC
** from commit
$ scrapy shell url
% view(response)
% link extraction
% fetch(link[0].url)
%
% view(response)
# Chrome Inspector copy selector then modify selector to more specific,
# like keep id selector, add classname to last node
% response.css()

# history commit workflow
scrapy        : new project (toscrape_book)
toscrape_book : add spider (books)
toscrape_book : update items.py (add BookDetailItem)
toscrape_book : update spider (books)
toscrapy_book : update spider (print counter)
toscrapy_book : add custom export fields
toscrapy_book : add BookPipeline
toscrapy_book : sample result
* selector
#+BEGIN_SRC python
# create selector from text
from scrapy.selector import Selector
text = '''
<html>
  <body>
    <h1>Hello  World</h1>
    <h1>Hello Scrapy</h1>

    <b>Hello python</b>

    <ul>
      <li>C++</li>
      <li>Java</li>
      <li>Python</li>
    </ul>
  </body>
</html>
'''
selector = Selector(text=text)

# create selector from response
from scrapy.selector import Selector
from scrapy.http import HtmlResponse

body = '''
<html>
  <body>
    <h1>Hello  World</h1>
    <h1>Hello Scrapy</h1>

    <b>Hello python</b>

    <ul>
      <li>C++</li>
      <li>Java</li>
      <li>Python</li>
    </ul>
  </body>
</html>
'''

response = HtmlResponse(url='http://www.example.com', body=body, encoding='utf8')
selector = Selector(response=response)
response.selector # TextResponse property "selector"

response.xpath('.//h1/text()').extract() # call selector.xpath inside (defined in TextResponse)
response.css('li::text').extract()       # call selector.css inside   (defined in TextResponse)
#+END_SRC

#+BEGIN_SRC python
# selection
selector_list = selector.xpath('//h1') # select all <h1> in document
# output
#
# [<Selector xpath='.//h1' data='<h1>Hello  World</h1>'>,
#  <Selector xpath='.//h1' data='<h1>Hello Scrapy</h1>'>]

for sel in selector_list:
    print(sel.xpath('./text()'))
# output
#
# [<Selector xpath='./text()' data='Hello  World'>]
# [<Selector xpath='./text()' data='Hello Scrapy'>]

selector_list.xpath('./text()')
# output
#
# [<Selector xpath='./text()' data='Hello  World'>,
#  <Selector xpath='./text()' data='Hello Scrapy'>]

selector.xpath('.//ul').css('li').xpath('./text()')
# output
#
# [<Selector xpath='./text()' data='C++ '>,
#  <Selector xpath='./text()' data='Java'>,
#  <Selector xpath='./text()' data='Python'>]
#+END_SRC

#+BEGIN_SRC python
# extract data using extract

sl = selector.xpath('.//li')
sl[0].extract()
# output:   '<li>C++</li>'

sl = selector.xpath('.//li/text()')
sl[0].extract()
# output:   'C++'

sl = selector.xpath('.//li/text()')
sl.extract()
# output:   ['C++', 'Java', 'Python']

sl = selector.xpath('.//b')
sl.extract_first()
# output:   '<b>Hello Python</b>'
#+END_SRC

#+BEGIN_SRC python
# extract data using re
text = '''
<ul>
  <li>Python 学习手册 <b>价格: 99.00 元</b></li>
  <li>Python 核心编程 <b>价格: 88.00 元</b></li>
  <li>Python 基础教程 <b>价格: 80.00 元</b></li>
</ul>
'''
selector = Selector(text=text)
selector.xpath('.//li/b/text()')
selector.xpath('.//li/b/text()').extract()
# output:  ['价格: 99.00 元', '价格: 88.00 元', '价格: 80.00 元']
selector.xpath('.//li/b/text()').re('\d+\.\d+')
# output:  ['99.00', '88.00', '80.00']
selector.xpath('.//li/b/text()').re_first('\d+\.\d+')
# output:  '99.00'
#+END_SRC
* XPath
| expression  | description                        |
|-------------+------------------------------------|
| /           | root (not node)                    |
| .           | current node                       |
| ..          | parent node                        |
| ELEMENT     | All children nodes named ELEMENT   |
| //ELEMENT   | All descendant nodes named element |
| *           | All nodes                          |
| text()      | text node                          |
| @ATTR       | select node's attribute named ATTR |
| @*          | select node's all attributes       |
| [predicate] | specify node                       |

#+BEGIN_SRC python
from scrapy.selector import Selector
from scrapy.http import HtmlResponse

body = '''
<html>
  <head>
    <base href="http://example.com/" />
    <title>Example website</title>
  </head>

  <body>
    <div id="images">
      <a href="image1.html">Name: Image 1 <br /><img class="thumb" src="image1.jpg" /><strong>tail</strong></a>
      <a href="image2.html">Name: Image 2 <br /><img class="thumb" src="image2.jpg" /></a>
      <a href="image3.html">Name: Image 3 <br /><img src="image3.jpg" /></a>
      <a href="image4.html">Name: Image 4 <br /><img src="image4.jpg" /></a>
      <a href="image5.html">Name: Image 5 <br /><img src="image5.jpg" /></a>
    </div>
  </body>
</html>
'''

response = HtmlResponse(url='http://www.example.com', body=body, encoding='utf8')

# ipython
# run xpath_demo.py

## selector example
response.xpath('/html')
response.xpath('/html/head')
response.xpath('/html/body/div/a')

## selector //
response.xpath('//a')
response.xpath('/html/body//img')

## selector text()
response.xpath('//a/text()').extract()

## selector *
response.xpath('/html/*')
response.xpath('/html/body/div//*')
response.xpath('//div/*/img')

## attribute selector
response.xpath('//img/@src')
response.xpath('//@href')

## index is 1-based
response.xpath('//a[1]/img/@*')

## selector current
sel = response.xpath('//a')[0]
sel.xpath('//img')  # wrong, this will select at the root, so the images is not single
sel.xpath('.//img') # correct

## selector parent
response.xpath('//img/..')

## selector predicate
response.xpath('//a[3]')
response.xpath('//a[last()]')
response.xpath('//a[position()<=3]')
response.xpath('//div[@id]')
response.xpath('//div[@id="images"]')

response.xpath('//img[contains(@class, "thumb")]')

## xpath function
response.xpath('string(/html/body/div/a)').extract()
# return ['Name: Image 1 tail']
response.xpath('/html/body/div/a[1]//text()').extract()
# return ['Name: Image 1 ', 'tail']
#+END_SRC
* CSS
| expression          | description                | example           |
|---------------------+----------------------------+-------------------|
| *                   | all elements               | *                 |
| E                   | element named E            | p                 |
| E1, E2              | elements named E1 or E2    | div, p            |
| E1 E2               | E1's descendant element E2 | div p             |
| E1 > E2             | E1's children E2           | div > p           |
| E1 + E2             | E1's sibling E2            | p + strong        |
| .class_name         | attribute class            | .info             |
| #id                 | attribute id               | #main             |
| [ATTR]              | attirubte named ATTR       | [href]            |
| [ATTR=VALUE]        | attribute ATTR with value  | [method=POST]     |
| [ATTR~=VALUE]       | attribute include value    | [class~=clearfix] |
| E:nth-child(n)      |                            |                   |
| E:nth-last-child(n) |                            |                   |
| E:first-child       |                            |                   |
| E:last-child        |                            |                   |
| E:empty             | element with nothing       | div:empty         |
| E::text             | element text node          |                   |

#+BEGIN_SRC python
from scrapy.selector import Selector
from scrapy.http import HtmlResponse

body = '''
<html>
  <head>
    <base href="http://example.com/" />
    <title>Example website</title>
  </head>
  <body>
    <div id="images-1" style="width: 1230px">
      <a href="image1.html">Name: Image 1 <br /><img src="image1.jpg" /></a>
      <a href="image2.html">Name: Image 2 <br /><img src="image2.jpg" /></a>
      <a href="image3.html">Name: Image 3 <br /><img src="image3.jpg" /></a>
    </div>
    <div id="images-2" class="small">
      <a href="image4.html">Name: Image 4 <br /><img src="image4.jpg" /></a>
      <a href="image5.html">Name: Image 5 <br /><img src="image5.jpg" /></a>
    </div>
  </body>
</html>
'''

response = HtmlResponse(url='http://www.example.com', body=body, encoding='utf8')

response.css('img')
response.css('base, title')
response.css('div img')
response.css('body > div')
response.css('[style]')
response.css('[id=images-1]')
response.css('div > a:nth-child(1)')
response.css('div:nth-child(2) > a:nth-child(1)')
response.css('div:first-child > a:last-child')
response.css('a::text')
response.css('a::attr(href)')
#+END_SRC
* exporter
** exporter class
check scrapy/exporters.py class inherited from BaseItemExporter

JsonItemExporter
JsonLinesItemExporter
CsvItemExporter
XmlItemExporter

PickleItemExporter
MarshalItemExporter

PprintItemExporter
PythonItemExporter
** export
在 导出 数据 时, 需向 Scrapy 爬虫 提供 以下 信息:
1. 导出 文件 路径
2. 导出 数据 格式

可以 通过 以下 两种 方式, 指定 爬虫 如何 导出 数据:
1. 通过 命令行 参数 指定 (command line)
   # -t csv is inferred from books.csv, so could be ommited
   $ scrapy crawl books -o books.csv
   # -t json is inferred from books.json, so could be ommited
   $ scrapy crawl books -o books.json

   $ scrapy crawl books -t csv -o books.data

   # when export files, you can use variable: %(name)s and %(time)s
   # %(name)s  -->  spider name
   # %(time)s  -->  file create time

   $ scrapy crawl books -o 'export_data/%(name)s/%(time)s.csv'
   $ scrapy crawl games -o 'export_data/%(name)s/%(time)s.csv'
   $ scrapy crawl news  -o 'export_data/%(name)s/%(time)s.csv'
   $ scrapy crawl books -o 'export_data/%(name)s/%(time)s.csv'

   $ tree export_data
   export_data/
   ├── books
   │  ├── 2017-03-06T02-31-57.csv
   │  └── 2017-06-07T04-45-13.csv
   ├── games
   │  └── 2017-04-05T01-43-01.csv
   └── news
      └── 2017-05-06T09-44-06.csv

   --output-format=FORMAT, -t FORMAT
     format to use for dumping items

   Defined Location:

     scrapy/settings/default_settings.py
       FEED_EXPORTERS_BASE

     user/settings.py
       FEED_EXPORTERS

2. 通过 配置文件 指定 (settings)
     user/settings.py
       FEED_URI             = 'export_data/%(name)s.data'
       FEED_FORMAT          = 'csv'
       FEED_EXPORT_ENCODING = 'utf-8'
       FEED_EXPORT_FIELDS   = [ 'name', 'author', 'price' ] # order matters
       FEED_EXPORTERS       = { 'excel': 'your_project_name.my_exporters.ExcelItemExporter' }
** define
#+BEGIN_SRC python
class JsonItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        pass

    def start_exporting(self):
        pass

    def finish_exporting(self):
        pass

    def export_item(self, item):
        raise NotImplementedError
#+END_SRC
** default settings
scrapy/utils/conf.py
  def feed_complete_default_values_from_settings
* ask
** [[https://stackoverflow.com/questions/34485789/scrapy-csv-output-without-header][Python - Scrapy: CSV output without header - Stack Overflow]]
** [[https://blog.csdn.net/yimingsilence/article/details/52119720][scrapy 爬虫 出现 Forbidden by robots.txt - William Zhao's notes]]
scrapy default_settings.py:
  ROBOTSTXT_OBEY

project settings.py:
  ROBOTSTXT_OBEY
* scrapy shell
$ scrapy shell url

request           -- 最近一次 Request 对象
response          -- 最近一次 Response 对象
fetch(req_or_url) -- 传入 Request 对象 url 字符串, 调用后 更新 变量 request 和 response
view(response)    -- 在 浏览器 显示 response 页面

可能 在 很多 时候, 使用 view 函数 打开 的 页面 和 在 浏览器 直接 输入 url 打开 的 页面 看起来 是 一样 的,
但 需要 知道 的 是, 前者 是 由 Scrapy 爬虫 下载 的 页面,
而 后者 是 由 浏览器 下载 的 页面,
有时 它们 是 不同 的.
在 进行 页面 分析 时, 使用 view 函数 更加 可靠
** detail
$ scrapy shell http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html
% view(response)

% sel = response.css('div.product_main')
% title = sel.xpath('./h1/text()').extract_first()
'A Light in the Attic'
% price = sel.css('p.price_color::text').extract_first()
'￡51.77'
% rate_level = sel.css('p.star-rating::attr(class)').re_first('star-rating ([A-Za-z]+)')
'Three'

% sel = response.css('table.table.table-striped')
% product_id = sel.xpath('(.//tr)[1]/td/text()').extract_first()
'a897fe39b1053632'
% in_stock = sel.xpath('(.//tr)[last()-1]/td/text()').re_first('\((\d+) available\)')
'22'
% comment_count = sel.xpath('(.//tr)[last()]/td/text()').extract_first()
'0'
** index page (continued from detail above)
% fetch('http://books.toscrape.com/')
% view(response)
%
% from scrapy.linkextractors import LinkExtractor
% le = LinkExtractor(restrict_css='article.product_pod')
% le.extract_links(response)
* pipeline
- FilesPipeline
- ImagesPipeline

我们 可以 将 这 两个 Item Pipeline 看作 特殊 的 下载器,
用户 使用 时,
只 需要 通过 item 的 一个 特殊 字段,
将 要 下载文件 或 图片 的 url 传递 给 它们,
它们 会 自动 将 文件 或 图片 下载 到 本地,
并 将 下载结果 信息 存入 item 的 另一个特殊字段,
以便 用户 在 导出文件 中 查阅

** FilesPipeline
#+BEGIN_SRC html
<html>
  <body>
    <div>
      <a href="/book/sg.pdf">下载《三国演义》</a>
      <a href="/book/shz.pdf">下载《水浒传》</a>
      <a href="/book/hlm.pdf">下载《红楼梦》</a>
      <a href="/book/xyj.pdf">下载《西游记》</a>
    </div>
  </body>
</html>
#+END_SRC

1. In file settings.py enable "FilesPipeline", before any other Item Pipelines normally

     ITEM_PIPELINES = { 'scrapy.pipelines.files.FilesPipeline': 1 }
     FILES_STORE    =   '/home/liushuo/Download/scrapy'

2. Put download urls into -> item['file_urls']
     #+BEGIN_SRC python
class DownloadBookSpider(scrapy.Spider):
    def parse(response):
        # define field file_urls in DownloadBookItem
        item = DownloadBookItem()

        item['file_urls'] = []

        for url in response.xpath('//a/@href').extract():
            download_url = response.urljoin(url)

            item['file_urls'].append(download_url)

        yield item
      #+END_SRC
3. 当 FilesPipeline 下载 完 item['file_urls'] 中 所有 文件 后,
   会将 各文件 的 下载结果 收集 到 另一个 列表, 赋给 item 的 files 字段 item['files']
   下载结果 包括 以下 内容:
     1. path:     文件 下载 到 本地 的 路径 (相对于 FILES_STORE 的 相对路径) (URL's sha1sum)
     2. checksum: 文件 的 校验和 (md5)
     3. url:      文件 的 url 地址
     4. status:   文件 下载状态, 比如 downloaded
** ImagesPipeline
settings.py:

     ITEM_PIPELINES = { 'scrapy.pipelines.images.ImagesPipeline: 1 }
     IMAGES_STORE   =   '/home/liushuo/Download/scrapy'

     1. Provide generate thumbnail:

       IMAGES_THUMBS = {
         'small': (50, 50),
         'big': (270, 270),
       }

       Example Downloaded images:

         [IMAGES_STORE]/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
         [IMAGES_STORE]/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
         [IMAGES_STORE]/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg

     2. Filter out small image:

          IMAGES_MIN_WIDTH = 110
          IMAGES_MIN_HEIGHT = 110

          开启 该 功能 后, 如果 下载 了 一张 105×200 图片, 该 图片 就会 被 抛弃掉,
          因为 宽度 不符合 标准

Item Field:
     image_urls (download urls)
     images     (downloaded images)
