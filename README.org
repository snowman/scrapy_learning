* concept
如果 把 框架 中 的 组件 比作 人体 的 各个 器官,
Request 和 Response 对象 是 血液, Item 则 是 代谢产物
* API
#+BEGIN_SRC python
req = Request(url[,
              callback,
              method='GET',
              headers,
              body,
              cookies,
              meta,
              encoding='utf-8',
              priority=0,
              dont_filter=False,
              errback])

import scrapy
request = scrapy.Request('http://books.toscrape.com/')
request2 = scrapy.Request('http://quotes.toscrape.com/', callback=self.parseItem)

在实际应用中, 我们几乎只调用Request的构造器创建对象,
但也可以根据需求访问Request对象的属性, 常用的有以下几个:

- url
- method
- headers
- body
- meta

这些属性和构造器参数相对应
#+END_SRC

#+BEGIN_SRC python
# response has 3 types: TextResponse, HtmlResponse, XmlResponse
#
# HtmlResponse
#
# property:
# - url, status, headers, body, text, encoding, request, meta, selector, xpath, css, urljoin

response.headers.get('Content-Type')
response.headers.getlist('Set-Cookie')

reponse.text = response.body.decode(response.encoding)

# resonse.meta 即response.request.meta
# 在构造Request对象时, 可将要传递给parse函数 的 信息 通过meta参数 传入;
# parse 函数处理响应时, 通过response.meta将信息取出
#+END_SRC
* workflow
实现 一个 Spider 子类 的 过程, 很像 是 完成 一系列 填空题:

Scrapy 框架 提出 以下 问题, 让 用户 在 Spider 子类 中 作答:
1. 爬虫 从 哪个 或 哪些 页面 开始 爬取?
2. 对于 一个 已下载 的 页面, 提取 其中 的 哪些 数据?
3. 爬取 完 当前 页面 后, 接下来 爬取 哪个 或 哪些 页面?

#+BEGIN_SRC python
class BooksSpider(scrapy.Spider):
    start_urls = ['http://books.toscrape.com/']

    def start_requests(self):
        for url in start_urls:
            yield scrapy.Request(url,
                                 callback=self.parse_book,
                                 headers={'User-Agent': 'Mozilla/5.0'},
                                 dont_filter=True)

    def parse_book(response):
        yield {}
        yield scrapy.Request(next_url, callback=self.parse_book)
#+END_SRC
