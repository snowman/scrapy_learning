* run
** Start spider
$ scrapy crawl books -o books.csv
* concept
如果 把 框架 中 的 组件 比作 人体 的 各个 器官,
Request 和 Response 对象 是 血液, Item 则 是 代谢产物
* API
#+BEGIN_SRC python
req = Request(url[,
              callback,
              method='GET',
              headers,
              body,
              cookies,
              meta,
              encoding='utf-8',
              priority=0,
              dont_filter=False,
              errback])

import scrapy
request = scrapy.Request('http://books.toscrape.com/')
request2 = scrapy.Request('http://quotes.toscrape.com/', callback=self.parseItem)

在实际应用中, 我们几乎只调用Request的构造器创建对象,
但也可以根据需求访问Request对象的属性, 常用的有以下几个:

- url
- method
- headers
- body
- meta

这些属性和构造器参数相对应
#+END_SRC

#+BEGIN_SRC python
# response has 3 types: TextResponse, HtmlResponse, XmlResponse
#
# HtmlResponse
#
# property:
# - url, status, headers, body, text, encoding, request, meta, selector, xpath, css, urljoin

response.headers.get('Content-Type')
response.headers.getlist('Set-Cookie')

reponse.text = response.body.decode(response.encoding)

# resonse.meta 即response.request.meta
# 在构造Request对象时, 可将要传递给parse函数 的 信息 通过meta参数 传入;
# parse 函数处理响应时, 通过response.meta将信息取出
#+END_SRC
* workflow
实现 一个 Spider 子类 的 过程, 很像 是 完成 一系列 填空题:

Scrapy 框架 提出 以下 问题, 让 用户 在 Spider 子类 中 作答:
1. 爬虫 从 哪个 或 哪些 页面 开始 爬取?
2. 对于 一个 已下载 的 页面, 提取 其中 的 哪些 数据?
3. 爬取 完 当前 页面 后, 接下来 爬取 哪个 或 哪些 页面?

#+BEGIN_SRC python
class BooksSpider(scrapy.Spider):
    start_urls = ['http://books.toscrape.com/']

    def start_requests(self):
        for url in start_urls:
            yield scrapy.Request(url,
                                 callback=self.parse_book,
                                 headers={'User-Agent': 'Mozilla/5.0'},
                                 dont_filter=True)

    def parse_book(response):
        yield {}
        yield scrapy.Request(next_url, callback=self.parse_book)
#+END_SRC
* selector
#+BEGIN_SRC python
# create selector from text
from scrapy.selector import Selector
text = '''
<html>
  <body>
    <h1>Hello  World</h1>
    <h1>Hello Scrapy</h1>

    <b>Hello python</b>

    <ul>
      <li>C++</li>
      <li>Java</li>
      <li>Python</li>
    </ul>
  </body>
</html>
'''
selector = Selector(text=text)

# create selector from response
from scrapy.selector import Selector
from scrapy.http import HtmlResponse

body = '''
<html>
  <body>
    <h1>Hello  World</h1>
    <h1>Hello Scrapy</h1>

    <b>Hello python</b>

    <ul>
      <li>C++</li>
      <li>Java</li>
      <li>Python</li>
    </ul>
  </body>
</html>
'''

response = HtmlResponse(url='http://www.example.com', body=body, encoding='utf8')
selector = Selector(response=response)
response.selector # TextResponse property "selector"

response.xpath('.//h1/text()').extract() # call selector.xpath inside (defined in TextResponse)
response.css('li::text').extract()       # call selector.css inside   (defined in TextResponse)
#+END_SRC

#+BEGIN_SRC python
# selection
selector_list = selector.xpath('//h1') # select all <h1> in document
# output
#
# [<Selector xpath='.//h1' data='<h1>Hello  World</h1>'>,
#  <Selector xpath='.//h1' data='<h1>Hello Scrapy</h1>'>]

for sel in selector_list:
    print(sel.xpath('./text()'))
# output
#
# [<Selector xpath='./text()' data='Hello  World'>]
# [<Selector xpath='./text()' data='Hello Scrapy'>]

selector_list.xpath('./text()')
# output
#
# [<Selector xpath='./text()' data='Hello  World'>,
#  <Selector xpath='./text()' data='Hello Scrapy'>]

selector.xpath('.//ul').css('li').xpath('./text()')
# output
#
# [<Selector xpath='./text()' data='C++ '>,
#  <Selector xpath='./text()' data='Java'>,
#  <Selector xpath='./text()' data='Python'>]
#+END_SRC

#+BEGIN_SRC python
# extract data using extract

sl = selector.xpath('.//li')
sl[0].extract()
# output:   '<li>C++</li>'

sl = selector.xpath('.//li/text()')
sl[0].extract()
# output:   'C++'

sl = selector.xpath('.//li/text()')
sl.extract()
# output:   ['C++', 'Java', 'Python']

sl = selector.xpath('.//b')
sl.extract_first()
# output:   '<b>Hello Python</b>'
#+END_SRC

#+BEGIN_SRC python
# extract data using re
text = '''
<ul>
  <li>Python 学习手册 <b>价格: 99.00 元</b></li>
  <li>Python 核心编程 <b>价格: 88.00 元</b></li>
  <li>Python 基础教程 <b>价格: 80.00 元</b></li>
</ul>
'''
selector = Selector(text=text)
selector.xpath('.//li/b/text()')
selector.xpath('.//li/b/text()').extract()
# output:  ['价格: 99.00 元', '价格: 88.00 元', '价格: 80.00 元']
selector.xpath('.//li/b/text()').re('\d+\.\d+')
# output:  ['99.00', '88.00', '80.00']
#+END_SRC
* XPath
| expression  | description                        |
|-------------+------------------------------------|
| /           | root (not node)                    |
| .           | current node                       |
| ..          | parent node                        |
| ELEMENT     | All children nodes named ELEMENT   |
| //ELEMENT   | All descendant nodes named element |
| *           | All nodes                          |
| text()      | text node                          |
| @ATTR       | select node's attribute named ATTR |
| @*          | select node's all attributes       |
| [predicate] | specify node                       |

#+BEGIN_SRC python
from scrapy.selector import Selector
from scrapy.http import HtmlResponse

body = '''
<html>
  <head>
    <base href="http://example.com/" />
    <title>Example website</title>
  </head>

  <body>
    <div id="images">
      <a href="image1.html">Name: Image 1 <br /><img class="thumb" src="image1.jpg" /><strong>tail</strong></a>
      <a href="image2.html">Name: Image 2 <br /><img class="thumb" src="image2.jpg" /></a>
      <a href="image3.html">Name: Image 3 <br /><img src="image3.jpg" /></a>
      <a href="image4.html">Name: Image 4 <br /><img src="image4.jpg" /></a>
      <a href="image5.html">Name: Image 5 <br /><img src="image5.jpg" /></a>
    </div>
  </body>
</html>
'''

response = HtmlResponse(url='http://www.example.com', body=body, encoding='utf8')

# ipython
# run xpath_demo.py

## selector example
response.xpath('/html')
response.xpath('/html/head')
response.xpath('/html/body/div/a')

## selector //
response.xpath('//a')
response.xpath('/html/body//img')

## selector text()
response.xpath('//a/text()').extract()

## selector *
response.xpath('/html/*')
response.xpath('/html/body/div//*')
response.xpath('//div/*/img')

## attribute selector
response.xpath('//img/@src')
response.xpath('//@href')

## index is 1-based
response.xpath('//a[1]/img/@*')

## selector current
sel = response.xpath('//a')[0]
sel.xpath('//img')  # wrong, this will select at the root, so the images is not single
sel.xpath('.//img') # correct

## selector parent
response.xpath('//img/..')

## selector predicate
response.xpath('//a[3]')
response.xpath('//a[last()]')
response.xpath('//a[position()<=3]')
response.xpath('//div[@id]')
response.xpath('//div[@id="images"]')

response.xpath('//img[contains(@class, "thumb")]')

## xpath function
response.xpath('string(/html/body/div/a)').extract()
# return ['Name: Image 1 tail']
response.xpath('/html/body/div/a[1]//text()').extract()
# return ['Name: Image 1 ', 'tail']
#+END_SRC
* CSS
| expression          | description                | example           |
|---------------------+----------------------------+-------------------|
| *                   | all elements               | *                 |
| E                   | element named E            | p                 |
| E1, E2              | elements named E1 or E2    | div, p            |
| E1 E2               | E1's descendant element E2 | div p             |
| E1 > E2             | E1's children E2           | div > p           |
| E1 + E2             | E1's sibling E2            | p + strong        |
| .class_name         | attribute class            | .info             |
| #id                 | attribute id               | #main             |
| [ATTR]              | attirubte named ATTR       | [href]            |
| [ATTR=VALUE]        | attribute ATTR with value  | [method=POST]     |
| [ATTR~=VALUE]       | attribute include value    | [class~=clearfix] |
| E:nth-child(n)      |                            |                   |
| E:nth-last-child(n) |                            |                   |
| E:first-child       |                            |                   |
| E:last-child        |                            |                   |
| E:empty             | element with nothing       | div:empty         |
| E::text             | element text node          |                   |

#+BEGIN_SRC python
from scrapy.selector import Selector
from scrapy.http import HtmlResponse

body = '''
<html>
  <head>
    <base href="http://example.com/" />
    <title>Example website</title>
  </head>
  <body>
    <div id="images-1" style="width: 1230px">
      <a href="image1.html">Name: Image 1 <br /><img src="image1.jpg" /></a>
      <a href="image2.html">Name: Image 2 <br /><img src="image2.jpg" /></a>
      <a href="image3.html">Name: Image 3 <br /><img src="image3.jpg" /></a>
    </div>
    <div id="images-2" class="small">
      <a href="image4.html">Name: Image 4 <br /><img src="image4.jpg" /></a>
      <a href="image5.html">Name: Image 5 <br /><img src="image5.jpg" /></a>
    </div>
  </body>
</html>
'''

response = HtmlResponse(url='http://www.example.com', body=body, encoding='utf8')

response.css('img')
response.css('base, title')
response.css('div img')
response.css('body > div')
response.css('[style]')
response.css('[id=images-1]')
response.css('div > a:nth-child(1)')
response.css('div:nth-child(2) > a:nth-child(1)')
response.css('div:first-child > a:last-child')
response.css('a::text')
#+END_SRC
* exporter
** exporter class
check scrapy/exporters.py class inherited from BaseItemExporter

JsonItemExporter
JsonLinesItemExporter
CsvItemExporter
XmlItemExporter

PickleItemExporter
MarshalItemExporter

PprintItemExporter
PythonItemExporter
** export
在 导出 数据 时, 需向 Scrapy 爬虫 提供 以下 信息:
1. 导出 文件 路径
2. 导出 数据 格式

可以 通过 以下 两种 方式, 指定 爬虫 如何 导出 数据:
1. 通过 命令行 参数 指定 (command line)
   # -t csv is inferred from books.csv, so could be ommited
   $ scrapy crawl books -o books.csv
   # -t json is inferred from books.json, so could be ommited
   $ scrapy crawl books -o books.json

   $ scrapy crawl books -t csv -o books.data

   # when export files, you can use variable: %(name)s and %(time)s
   # %(name)s  -->  spider name
   # %(time)s  -->  file create time

   $ scrapy crawl books -o 'export_data/%(name)s/%(time)s.csv'
   $ scrapy crawl games -o 'export_data/%(name)s/%(time)s.csv'
   $ scrapy crawl news  -o 'export_data/%(name)s/%(time)s.csv'
   $ scrapy crawl books -o 'export_data/%(name)s/%(time)s.csv'

   $ tree export_data
   export_data/
   ├── books
   │  ├── 2017-03-06T02-31-57.csv
   │  └── 2017-06-07T04-45-13.csv
   ├── games
   │  └── 2017-04-05T01-43-01.csv
   └── news
      └── 2017-05-06T09-44-06.csv

   --output-format=FORMAT, -t FORMAT
     format to use for dumping items

   Defined Location:

     scrapy/settings/default_settings.py
       FEED_EXPORTERS_BASE

     user/settings.py
       FEED_EXPORTERS

2. 通过 配置文件 指定 (settings)
     user/settings.py
       FEED_URI             = 'export_data/%(name)s.data'
       FEED_FORMAT          = 'csv'
       FEED_EXPORT_ENCODING = 'utf-8'
       FEED_EXPORT_FIELDS   = [ 'name', 'author', 'price' ] # order matters
       FEED_EXPORTERS       = { 'excel': 'your_project_name.my_exporters.ExcelItemExporter' }
** define
#+BEGIN_SRC python
class JsonItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        pass

    def start_exporting(self):
        pass

    def finish_exporting(self):
        pass

    def export_item(self, item):
        raise NotImplementedError
#+END_SRC
** default settings
scrapy/utils/conf.py
  def feed_complete_default_values_from_settings
* ask
** [[https://stackoverflow.com/questions/34485789/scrapy-csv-output-without-header][Python - Scrapy: CSV output without header - Stack Overflow]]
